#!/usr/bin/env python3
# Written by Andrea Giani
"""
dna_analysis_ai.py
Quantum Genetics Accelerator - AI Analysis Module

This module provides functionalities for analyzing data generated by
dna_sequence_generator.py, results from FPGA simulations (placeholder),
and outputs from quantum_dna_simulator.py. It includes:
- Data loading and preprocessing.
- Feature Engineering for Machine Learning.
- Classification for pattern recognition (e.g., aging).
- Clustering for unsupervised mutation analysis.
- Comparison and validation between different data sources.
"""

import json
import pandas as pd
import numpy as np
from typing import List, Dict, Union, Any, Tuple
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, silhouette_score, adjusted_rand_score
from scipy.stats import entropy as shannon_entropy # For Shannon entropy

# DNA base definition for consistency with other modules
class DNABase:
    A = 0
    T = 1
    G = 2
    C = 3
    # Mapping for string to numeric value conversion
    _str_to_int = {'A': 0, 'T': 1, 'G': 2, 'C': 3}
    _int_to_str = {0: 'A', 1: 'T', 2: 'G', 3: 'C'}

    @staticmethod
    def to_int(base_str: str) -> int:
        return DNABase._str_to_int.get(base_str.upper())

    @staticmethod
    def to_str(base_int: int) -> str:
        return DNABase._int_to_str.get(base_int)

class DNAAnalysisAI:
    """
    Class for DNA data analysis, Machine Learning, and validation.
    """
    def __init__(self):
        self.data_generated = None
        self.data_quantum_sim = None
        self.data_fpga = None # Placeholder for FPGA data

    def load_generated_data(self, file_path: str):
        """
        Loads generated data from dna_sequence_generator.py.
        """
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            self.data_generated = pd.DataFrame(data['sequences'])
            print(f"Generated data loaded from: {file_path}")
        except FileNotFoundError:
            print(f"Error: File not found at {file_path}")
        except json.JSONDecodeError:
            print(f"Error: Could not decode JSON from {file_path}")

    def load_quantum_sim_data(self, file_path: str):
        """
        Loads simulation results from quantum_dna_simulator.py.
        """
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            self.data_quantum_sim = pd.DataFrame(data['evolution_results'])
            # Convert base lists to strings for easier analysis
            self.data_quantum_sim['initial_sequence_str'] = self.data_quantum_sim['initial_sequence'].apply(lambda x: ''.join([DNABase.to_str(b) for b in x]))
            self.data_quantum_sim['final_sequence_str'] = self.data_quantum_sim['final_sequence'].apply(lambda x: ''.join([DNABase.to_str(b) for b in x]))
            print(f"Quantum simulation data loaded from: {file_path}")
        except FileNotFoundError:
            print(f"Error: File not found at {file_path}")
        except json.JSONDecodeError:
            print(f"Error: Could not decode JSON from {file_path}")

    def load_fpga_data(self, file_path: str):
        """
        Loads processed results from FPGA.
        NOTE: This is a placeholder. The FPGA file format needs to be defined.
        Assumes a JSON with 'id', 'processed_dna_hex', 'entropy_measure', 'mutation_count'.
        """
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            self.data_fpga = pd.DataFrame(data['fpga_results'])
            # Convert hexadecimal/binary representation to DNA base string if necessary
            # self.data_fpga['processed_dna_str'] = self.data_fpga['processed_dna_hex'].apply(self._hex_to_dna_str)
            print(f"FPGA data loaded from: {file_path}")
        except FileNotFoundError:
            print(f"Error: File not found at {file_path}")
        except json.JSONDecodeError:
            print(f"Error: Could not decode JSON from {file_path}")

    def _hex_to_dna_str(self, hex_str: str) -> str:
        """Converts a hexadecimal string (FPGA) to a DNA base string."""
        binary_str = bin(int(hex_str, 16))[2:].zfill(len(hex_str) * 4)
        dna_seq = []
        for i in range(0, len(binary_str), 2):
            base_val = int(binary_str[i:i+2], 2)
            dna_seq.append(DNABase.to_str(base_val))
        return ''.join(dna_seq)

    def sequence_to_one_hot(self, sequence: str) -> np.ndarray:
        """
        Converts a DNA sequence string into a one-hot encoding representation.
        Ex: "ATGC" -> [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]
        """
        mapping = {'A': [1,0,0,0], 'T': [0,1,0,0], 'G': [0,0,1,0], 'C': [0,0,0,1]}
        try:
            return np.array([mapping[base] for base in sequence]).flatten()
        except KeyError as e:
            print(f"Error: Invalid DNA base found in sequence '{sequence}': {e}")
            return np.array([]) # Returns an empty array on error

    def generate_kmer_features(self, sequence: str, k: int = 2) -> Dict[str, int]:
        """
        Generates a dictionary of k-mer counts for a given sequence.
        """
        kmers = [sequence[i:i+k] for i in range(len(sequence) - k + 1)]
        return dict(Counter(kmers))

    def extract_features(self, df: pd.DataFrame, sequence_col: str, prefix: str = "") -> pd.DataFrame:
        """
        Extracts features from the DataFrame, including one-hot encoding and k-mers.
        Adds a prefix to feature column names.
        """
        features_list = []
        for _, row in df.iterrows():
            sequence_str = ''.join(row[sequence_col]) if isinstance(row[sequence_col], list) else row[sequence_col]
            
            one_hot_vec = self.sequence_to_one_hot(sequence_str)
            kmer_counts = self.generate_kmer_features(sequence_str)
            
            # Converts k-mers to a flat format for the DataFrame
            flat_kmer_counts = {f'kmer_{k}': v for k, v in kmer_counts.items()}
            
            row_features = {
                f'{prefix}sequence_length': len(sequence_str),
                f'{prefix}gc_content': row.get('gc_content', 0.0), # Use .get to avoid KeyError if column doesn't exist
                f'{prefix}mutation_count': row.get('mutation_count', 0),
                f'{prefix}entropy': row.get('entropy', 0.0),
                f'{prefix}aging_factor': row.get('aging_factor', 0.0),
                **flat_kmer_counts # Adds k-mers
            }
            # Adds one-hot features
            for i, val in enumerate(one_hot_vec):
                row_features[f'{prefix}one_hot_{i}'] = val
            
            features_list.append(row_features)
        
        # Creates a DataFrame from extracted features
        features_df = pd.DataFrame(features_list)
        # Fills NaN values (due to k-mers not present in all sequences) with 0
        features_df = features_df.fillna(0)
        return features_df

    def prepare_ml_data(self, df: pd.DataFrame, target_col: str, feature_cols: List[str] = None):
        """
        Prepares data for ML models: selects features, handles missing values, scales.
        """
        if feature_cols is None:
            # Selects all numeric columns except the target column
            feature_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col != target_col]

        X = df[feature_cols]
        y = df[target_col]

        # Handling missing values (e.g., filling with mean)
        X = X.fillna(X.mean())

        # Encode labels if they are strings
        if pd.api.types.is_string_dtype(y):
            le = LabelEncoder()
            y = le.fit_transform(y)
            self.label_encoder = le # Saves the encoder for future decoding
        else:
            self.label_encoder = None

        # Feature scaling
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        self.scaler = scaler # Saves the scaler for transforming new data

        return X_scaled, y

    def perform_classification(self, features: np.ndarray, labels: np.ndarray, test_size: float = 0.2):
        """
        Performs classification using RandomForestClassifier.
        """
        if len(features) == 0 or len(labels) == 0:
            print("No data for classification.")
            return None, None

        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=test_size, random_state=42)

        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)

        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print(f"Classifier Accuracy: {accuracy:.4f}")

        return model, accuracy

    def perform_clustering(self, features: np.ndarray, n_clusters: int = 3):
        """
        Performs clustering using KMeans.
        """
        if len(features) == 0:
            print("No data for clustering.")
            return None, None

        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(features)

        # Calculate silhouette score if there are at least 2 clusters and more than 1 sample
        if n_clusters > 1 and len(features) > 1:
            try:
                silhouette = silhouette_score(features, clusters)
                print(f"Silhouette Score: {silhouette:.4f}")
            except Exception as e:
                print(f"Could not calculate Silhouette Score: {e}")
                silhouette = None
        else:
            silhouette = None

        return clusters, silhouette

    def compare_simulations(self):
        """
        Compares results from different simulations (Generated, Quantum Sim, FPGA).
        Requires data to have been loaded.
        """
        if self.data_generated is None and self.data_quantum_sim is None and self.data_fpga is None:
            print("No data available for comparison. Load data first.")
            return

        print("\n--- Simulation Comparison ---")

        # Entropy Comparison
        plt.figure(figsize=(15, 6))
        if self.data_generated is not None:
            sns.histplot(self.data_generated['entropy'], color='blue', label='Generated (Shannon Entropy)', kde=True, alpha=0.6)
        if self.data_quantum_sim is not None:
            sns.histplot(self.data_quantum_sim['final_entropy'], color='green', label='Quantum Sim (Von Neumann Entropy)', kde=True, alpha=0.6)
        if self.data_fpga is not None:
            # Assumes 'entropy_measure' is the FPGA entropy column
            sns.histplot(self.data_fpga['entropy_measure'], color='red', label='FPGA (Simplified Entropy)', kde=True, alpha=0.6)
        plt.title('Entropy Distribution Across Different Simulations')
        plt.xlabel('Entropy Value')
        plt.ylabel('Frequency')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()

        # Mutation Count Comparison (if applicable)
        plt.figure(figsize=(15, 6))
        if self.data_generated is not None:
            sns.histplot(self.data_generated['mutation_count'], color='blue', label='Generated (Mutation Count)', kde=True, alpha=0.6)
        if self.data_fpga is not None:
            # Assumes 'mutation_count' is the FPGA mutation count column
            sns.histplot(self.data_fpga['mutation_count'], color='red', label='FPGA (Mutation Count)', kde=True, alpha=0.6)
        plt.title('Mutation Count Distribution Across Different Simulations')
        plt.xlabel('Mutation Count')
        plt.ylabel('Frequency')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()

        # Fidelity Comparison (if available from quantum simulator)
        if self.data_quantum_sim is not None and 'fidelity' in self.data_quantum_sim.columns:
            plt.figure(figsize=(10, 6))
            sns.histplot(self.data_quantum_sim['fidelity'], color='purple', label='Quantum Sim Fidelity', kde=True, alpha=0.7)
            plt.title('Quantum Simulation Fidelity Distribution')
            plt.xlabel('Fidelity Score')
            plt.ylabel('Frequency')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.show()

        # Further comparisons can be added here (e.g., sequence similarity, PCA)

# Main function for execution
def main():
    ai_analyzer = DNAAnalysisAI()

    # --- 1. Data Loading (Ensure these files exist or create dummy ones) ---
    # To test, you can generate these files using your scripts:
    # python dna_sequence_generator.py --batch-size 100 --progression --output generated_dna_data.json
    # python quantum_dna_simulator.py --output quantum_sim_results.json
    # For FPGA, you'll need to create an example JSON file or extract it from Verilog simulation.

    ai_analyzer.load_generated_data('generated_dna_data.json')
    ai_analyzer.load_quantum_sim_data('quantum_sim_results.json')
    
    # Create a dummy file for FPGA results for testing
    # In a real integration, this would be generated by your Verilog testbench
    dummy_fpga_data = {
        'fpga_results': [
            {'id': 'fpga_seq_0000', 'processed_dna_hex': 'A3C5', 'entropy_measure': 12000, 'mutation_count': 5},
            {'id': 'fpga_seq_0001', 'processed_dna_hex': 'F1E2', 'entropy_measure': 10500, 'mutation_count': 8},
            {'id': 'fpga_seq_0002', 'processed_dna_hex': 'B9D7', 'entropy_measure': 13000, 'mutation_count': 3},
            {'id': 'fpga_seq_0003', 'processed_dna_hex': '2A4C', 'entropy_measure': 11000, 'mutation_count': 6},
            {'id': 'fpga_seq_0004', 'processed_dna_hex': '7E8F', 'entropy_measure': 9500, 'mutation_count': 10},
        ]
    }
    with open('fpga_processed_dna.json', 'w') as f:
        json.dump(dummy_fpga_data, f, indent=2)
    ai_analyzer.load_fpga_data('fpga_processed_dna.json')


    # --- 2. Feature Engineering and ML Data Preparation ---
    if ai_analyzer.data_generated is not None:
        print("\n--- Generated Data Analysis ---")
        # Extract features from the 'mutated_sequence' column
        # Convert base lists to strings before extracting features
        ai_analyzer.data_generated['mutated_sequence_str'] = ai_analyzer.data_generated['mutated_sequence'].apply(lambda x: ''.join(x))
        generated_features_df = ai_analyzer.extract_features(
            ai_analyzer.data_generated,
            sequence_col='mutated_sequence_str',
            prefix='gen_'
        )
        
        # Combine extracted features with relevant original columns
        ml_data_generated = pd.concat([ai_analyzer.data_generated[['aging_factor', 'mutation_count', 'entropy']], generated_features_df], axis=1)
        
        # For classification, we might want to discretize the aging_factor
        # Example: 0-0.3 -> 'Low', 0.3-0.6 -> 'Medium', >0.6 -> 'High'
        bins = [0, 0.3, 0.6, 1.0]
        labels = ['Low', 'Medium', 'High']
        ml_data_generated['aging_category'] = pd.cut(ml_data_generated['aging_factor'], bins=bins, labels=labels, include_lowest=True)
        
        # Prepare data for classification
        # Select only numeric columns for features
        numeric_cols = [col for col in ml_data_generated.columns if pd.api.types.is_numeric_dtype(ml_data_generated[col])]
        # Remove the target column from the feature list if present
        feature_cols_for_ml = [col for col in numeric_cols if col not in ['aging_factor', 'mutation_count', 'entropy']] # Also exclude these if they are already in the target or not desired as direct features
        
        X_gen, y_gen = ai_analyzer.prepare_ml_data(ml_data_generated, target_col='aging_category', feature_cols=feature_cols_for_ml)
        
        # --- 3. Classification (Predicting Aging Category) ---
        print("\n--- Performing Classification (Generated Data) ---")
        classifier_model, accuracy = ai_analyzer.perform_classification(X_gen, y_gen)
        if classifier_model:
            print(f"Trained Classifier Model with accuracy: {accuracy:.4f}")
            # You can save the model here: joblib.dump(classifier_model, 'aging_classifier.pkl')

        # --- 4. Clustering (Finding Natural Groups) ---
        print("\n--- Performing Clustering (Generated Data) ---")
        # Use the same features, but without the target column
        clusters_gen, silhouette_gen = ai_analyzer.perform_clustering(X_gen, n_clusters=3)
        if clusters_gen is not None:
            ml_data_generated['cluster'] = clusters_gen
            print("First 5 rows with cluster assignment:")
            print(ml_data_generated[['aging_category', 'cluster', 'mutation_count', 'entropy']].head())
            
            # Cluster visualization (e.g., PCA for dimensionality reduction)
            from sklearn.decomposition import PCA
            pca = PCA(n_components=2)
            principal_components = pca.fit_transform(X_gen)
            plt.figure(figsize=(10, 8))
            sns.scatterplot(x=principal_components[:, 0], y=principal_components[:, 1], hue=clusters_gen, palette='viridis', legend='full', alpha=0.7)
            plt.title('DNA Sequence Clustering (PCA 2D)')
            plt.xlabel('Principal Component 1')
            plt.ylabel('Principal Component 2')
            plt.show()

    # --- 5. Simulation Comparison ---
    ai_analyzer.compare_simulations()

    print("\nAI Analysis completed.")

if __name__ == "__main__":
    main()
